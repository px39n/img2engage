{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b938f9ec-edab-40e4-82ba-9faf52ebb981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:35:52.198910300Z",
     "start_time": "2023-10-26T14:35:52.177910500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d17b1b9",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Train End to End Gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41861da-940a-4fb9-bbba-7cc9d1f72725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(224*224*3, 1024)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "model2 = MLP()\n",
    "model2.initialize_weights()\n",
    "# Load a pre-trained ResNet-18 model and modify the final layer\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "#model=model2\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00001)\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Define your desired number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(\"start epoch :\"+str(epoch))\n",
    "    # Add a progress bar for the training loop\n",
    "\n",
    "    for images, gazes in tqdm(train_loader): # train_progress_bar:\n",
    "        images = images.to(device)  # Convert one-hot to class index\n",
    "        gazes=torch.argmax(gazes, dim=1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, gazes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Update the progress bar\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_epochs == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_dir, f\"epoch_{epoch+1}.ckpt\"))\n",
    "\n",
    "    # Validation loop\n",
    "    if (epoch + 1) % validate_epochs == 0:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Add a progress bar for the validation loop\n",
    "        valid_progress_bar = tqdm(valid_loader, desc='Validating', leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, gazes in valid_progress_bar:\n",
    "                images, gazes = images.to(device), torch.argmax(gazes, dim=1).to(device)  # Convert one-hot to class index\n",
    "\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += gazes.size(0)\n",
    "                correct += (predicted == gazes).sum().item()\n",
    "\n",
    "        print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a50a18",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Train Depth+Img Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d71d6c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T15:12:16.129668200Z",
     "start_time": "2023-10-26T15:08:02.233606500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch :0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [01:23<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.256945919680905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   5%|▌         | 1/19 [00:01<00:18,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|█         | 2/19 [00:02<00:17,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 3/19 [00:03<00:17,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  21%|██        | 4/19 [00:04<00:16,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  26%|██▋       | 5/19 [00:05<00:15,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  32%|███▏      | 6/19 [00:06<00:13,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  37%|███▋      | 7/19 [00:07<00:12,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 8/19 [00:08<00:11,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 9/19 [00:10<00:12,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  53%|█████▎    | 10/19 [00:11<00:11,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  58%|█████▊    | 11/19 [00:12<00:09,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  63%|██████▎   | 12/19 [00:13<00:07,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  68%|██████▊   | 13/19 [00:14<00:06,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  74%|███████▎  | 14/19 [00:15<00:05,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  79%|███████▉  | 15/19 [00:16<00:04,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  84%|████████▍ | 16/19 [00:17<00:03,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  89%|████████▉ | 17/19 [00:18<00:02,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  95%|█████████▍| 18/19 [00:19<00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 6, 224, 224])\n",
      "Validation Accuracy: 26.94214876033058%\n",
      "start epoch :1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [01:20<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.1061678157224284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   5%|▌         | 1/19 [00:00<00:17,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|█         | 2/19 [00:01<00:16,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 3/19 [00:03<00:16,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  21%|██        | 4/19 [00:04<00:15,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  26%|██▋       | 5/19 [00:05<00:14,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  32%|███▏      | 6/19 [00:06<00:13,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  37%|███▋      | 7/19 [00:07<00:12,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 8/19 [00:08<00:11,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 9/19 [00:09<00:10,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  53%|█████▎    | 10/19 [00:10<00:09,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  58%|█████▊    | 11/19 [00:11<00:08,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  63%|██████▎   | 12/19 [00:12<00:07,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  68%|██████▊   | 13/19 [00:13<00:06,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  74%|███████▎  | 14/19 [00:14<00:05,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  79%|███████▉  | 15/19 [00:15<00:04,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  84%|████████▍ | 16/19 [00:16<00:03,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  89%|████████▉ | 17/19 [00:17<00:02,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  95%|█████████▍| 18/19 [00:18<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 6, 224, 224])\n",
      "Validation Accuracy: 25.950413223140497%\n",
      "start epoch :2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 47/77 [00:50<00:32,  1.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    125\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 127\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Update the progress bar\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch=32\n",
    "data_path = r\"D:\\Datasets\\Talis_frames15_v2\"  # Provide your data path here\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "validate_epochs = 1  # Example: validate every epoch\n",
    "save_epochs = 2     # Example: save the model every 2 epochs\n",
    "ckpt_dir = r\"D:\\Datasets\\Talis_frames15_v2\\checkpoints\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Randomly flip the image vertically\n",
    "    transforms.RandomRotation(degrees=15),  # Randomly rotate the image by up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly change the brightness, contrast, saturation, and hue\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.75, 1.33)),  # Randomly crop and resize the image\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "class GazeDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            data_path (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        # Create a dictionary mapping each unique gaze value to a unique integer\n",
    "        self.gaze_to_int = {gaze: idx for idx, gaze in enumerate(self.data_frame['gaze'].unique())}\n",
    "        self.num_classes = len(self.gaze_to_int)\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_path, self.data_frame['imgID'][idx])  # Assuming imgID is in the first column\n",
    "        image = Image.open(img_name)\n",
    "        dep_name = os.path.join(self.data_path+\"_depth\", self.data_frame['imgID'][idx])\n",
    "        depth = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            depth = transform2(depth)\n",
    "        gaze = self.data_frame['gaze'][idx]   # Assuming gaze is in the second column\n",
    "        gaze_idx = self.gaze_to_int[gaze]\n",
    "\n",
    "        # Convert gaze_idx to one-hot encoded vector\n",
    "        one_hot_gaze = torch.zeros(self.num_classes)\n",
    "        one_hot_gaze[gaze_idx] = 1\n",
    "        return image, depth, one_hot_gaze\n",
    "\n",
    "# Create datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming that your CSV file has two columns: 'imgID' for image file names and 'gaze' for gaze labels\n",
    "data_frame = pd.read_csv('D:\\Datasets\\Talis_frames15_v2\\labels_and_features_TRAIN.csv')\n",
    "\n",
    "# Count the frequency of each class\n",
    "class_sample_counts = data_frame['gaze'].value_counts().sort_index().to_numpy()\n",
    "# Compute weights for each class\n",
    "weights = 1.0 / class_sample_counts\n",
    "# Create a weight for each sample in the dataset\n",
    "sample_weights = weights[data_frame['gaze'].replace({gaze: idx for idx, gaze in enumerate(data_frame['gaze'].unique())}).to_numpy()]\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = GazeDataset(csv_file='D:\\Datasets\\Talis_frames15_v2\\labels_and_features_TRAIN.csv', data_path=data_path, transform=transform)\n",
    "valid_dataset = GazeDataset(csv_file='D:\\Datasets\\Talis_frames15_v2\\labels_and_features_VAL.csv', data_path=data_path, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, sampler=sampler, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=False, num_workers=0)\n",
    "\n",
    "# Load a pre-trained ResNet-18 model and modify the final layer\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "model.conv1 = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "#model=model2\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00001)\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Define your desired number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(\"start epoch :\"+str(epoch))\n",
    "    # Add a progress bar for the training loop\n",
    "\n",
    "    for images,depth, gazes in tqdm(train_loader): # train_progress_bar:\n",
    "        inputs = torch.cat([images, depth], dim=1)\n",
    "        # print(inputs.shape)\n",
    "        inputs, gazes = inputs.to(device), torch.argmax(gazes, dim=1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, gazes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Update the progress bar\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_epochs == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_dir, f\"epoch_{epoch+1}.ckpt\"))\n",
    "\n",
    "    # Validation loop\n",
    "    if (epoch + 1) % validate_epochs == 0:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Add a progress bar for the validation loop\n",
    "        valid_progress_bar = tqdm(valid_loader, desc='Validating', leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, depth, gazes in valid_progress_bar:\n",
    "                inputs = torch.cat([images, depth], dim=1)\n",
    "                inputs, gazes = inputs.to(device), torch.argmax(gazes, dim=1).to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += gazes.size(0)\n",
    "                correct += (predicted == gazes).sum().item()\n",
    "\n",
    "        print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6314c9-40ba-4bc3-875f-e580aa89f1c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:53:57.296589500Z",
     "start_time": "2023-10-26T14:53:57.265589600Z"
    }
   },
   "source": [
    "# Train Depth+Img+Head+Eye Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad723b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch :0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [02:35<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.341203099721438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 47.27272727272727%\n",
      "start epoch :1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [02:28<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.2730936214521333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 50.082644628099175%\n",
      "start epoch :2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▋                                                                       | 10/77 [00:19<02:08,  1.92s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch=32\n",
    "data_path = r\"C:\\Datasets\\Engagement\"  # Provide your data path here\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "validate_epochs = 1  # Example: validate every epoch\n",
    "save_epochs = 2     # Example: save the model every 2 epochs\n",
    "ckpt_dir = r\"C:\\Datasets\\Engagement\\checkpoints\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally\n",
    "    #transforms.RandomVerticalFlip(p=0.5),  # Randomly flip the image vertically\n",
    "    #transforms.RandomRotation(degrees=15),  # Randomly rotate the image by up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly change the brightness, contrast, saturation, and hue\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.75, 1.33)),  # Randomly crop and resize the image\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform3 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "class GazeDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            data_path (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        # Create a dictionary mapping each unique gaze value to a unique integer\n",
    "        self.gaze_to_int ={3: 0, 2: 1, 1: 0, 4: 1}\n",
    "        self.num_classes = 2\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_path+\"\\image_original\", self.data_frame['imgID'][idx])  # Assuming imgID is in the first column\n",
    "        image = Image.open(img_name)\n",
    "        dep_name = os.path.join(self.data_path+\"\\depth_intermediate\", self.data_frame['imgID'][idx])\n",
    "        depth = Image.open(dep_name)\n",
    "        gaze_name = os.path.join(self.data_path+\"\\predict_heatmap\", self.data_frame['imgID'][idx])\n",
    "        gaze_img = Image.open(gaze_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            depth = transform2(depth)\n",
    "            gaze_img = transform2(gaze_img)\n",
    "\n",
    "        \n",
    "        gaze = self.data_frame['gaze'][idx]   # Assuming gaze is in the second column\n",
    "        gaze_idx = self.gaze_to_int[gaze]\n",
    "\n",
    "        # Convert gaze_idx to one-hot encoded vector\n",
    "        one_hot_gaze = torch.zeros(self.num_classes)\n",
    "        one_hot_gaze[gaze_idx] = 1\n",
    "        return image, depth,gaze_img, one_hot_gaze\n",
    "\n",
    "# Create datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming that your CSV file has two columns: 'imgID' for image file names and 'gaze' for gaze labels\n",
    "data_frame = pd.read_csv('C:\\Datasets\\labels_and_features_TRAIN.csv')\n",
    "\n",
    "\n",
    "train_dataset = GazeDataset(csv_file='C:\\Datasets\\labels_and_features_TRAIN.csv', data_path=data_path, transform=transform)\n",
    "valid_dataset = GazeDataset(csv_file='C:\\Datasets\\labels_and_features_VAL.csv', data_path=data_path, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=False, num_workers=0)\n",
    "\n",
    "# Load a pre-trained ResNet-18 model and modify the final layer\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "model.conv1 = nn.Conv2d(7, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "#model=model2\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00001)\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Define your desired number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(\"start epoch :\"+str(epoch))\n",
    "    # Add a progress bar for the training loop\n",
    "\n",
    "    for images,depth, gaze, gazes in tqdm(train_loader): # train_progress_bar:\n",
    "\n",
    "        inputs = torch.cat([images, depth, gaze], dim=1)\n",
    "        # print(inputs.shape)\n",
    "        inputs, gazes = inputs.to(device), torch.argmax(gazes, dim=1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, gazes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Update the progress bar\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_epochs == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_dir, f\"epoch_{epoch+1}.ckpt\"))\n",
    "\n",
    "    # Validation loop\n",
    "    if (epoch + 1) % validate_epochs == 0:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Add a progress bar for the validation loop\n",
    "        valid_progress_bar = tqdm(valid_loader, desc='Validating', leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, depth, gaze, gazes in valid_progress_bar:\n",
    "                inputs = torch.cat([images, depth, gaze ], dim=1)\n",
    "                inputs, gazes = inputs.to(device), torch.argmax(gazes, dim=1).to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += gazes.size(0)\n",
    "                correct += (predicted == gazes).sum().item()\n",
    "\n",
    "        print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687b1a1-8277-47f0-92ba-8bfd4ea4983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Function to plot images\n",
    "def plot_images(image, depth, gaze, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(transforms.ToPILImage()(image))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(transforms.ToPILImage()(depth))\n",
    "    plt.title(\"Depth Image\")\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(transforms.ToPILImage()(gaze))\n",
    "    plt.title(\"Gaze Image\")\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Randomly pick 3 images from the validation dataset\n",
    "random_indices = random.sample(range(len(valid_dataset)), 3)\n",
    "\n",
    "for idx in random_indices:\n",
    "    image, depth, gaze, label = valid_dataset[idx]\n",
    "    input_tensor = torch.cat([image.unsqueeze(0), depth.unsqueeze(0), gaze.unsqueeze(0)], dim=1)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "    # Plotting\n",
    "    plot_images(image, depth, gaze, f\"Label: {label}, Prediction: {predicted.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "356ed90d-7ab8-4445-bbc8-b4260d63c4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0, 2: 1, 1: 2, 4: 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.gaze_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dbe541-a138-40b5-bbf5-cc9a3ca20a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02237e74-d6b7-4a63-b535-5ab0ca022d5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Gaze Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de1e204-5912-406e-b3d6-c631e6f70e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch :0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [02:48<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8281658988494378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 50.247933884297524%\n",
      "start epoch :1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [02:50<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.780536125232647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 46.611570247933884%\n",
      "start epoch :2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [02:52<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.7447078924674493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.074380165289256%\n",
      "start epoch :3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [02:57<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.717821190883587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.900826446280995%\n",
      "start epoch :4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [02:56<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.6976761345739488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 55.53719008264463%\n",
      "start epoch :5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [02:49<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.6824425149273563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 57.52066115702479%\n",
      "start epoch :6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [02:48<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.6708717919015265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 57.85123966942149%\n",
      "start epoch :7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [06:37<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.6620415859408193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 58.51239669421488%\n",
      "start epoch :8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [03:09<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.6552678571118937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 59.83471074380165%\n",
      "start epoch :9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 77/77 [03:18<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.6499900825611957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 59.33884297520661%\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch=32\n",
    "data_path = r\"C:\\Datasets\\Engagement\"  # Provide your data path here\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "validate_epochs = 1  # Example: validate every epoch\n",
    "save_epochs = 2     # Example: save the model every 2 epochs\n",
    "ckpt_dir = r\"C:\\Datasets\\Engagement\\checkpoints\"\n",
    "\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "])\n",
    "class GazeDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            data_path (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        # Create a dictionary mapping each unique gaze value to a unique integer\n",
    "        self.gaze_to_int ={3: 0, 2: 1, 1: 0, 4: 1}\n",
    "        self.num_classes = 2\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_path+\"\\image_original\", self.data_frame['imgID'][idx])  # Assuming imgID is in the first column\n",
    "        image = Image.open(img_name)\n",
    "        dep_name = os.path.join(self.data_path+\"\\depth_intermediate\", self.data_frame['imgID'][idx])\n",
    "        depth = Image.open(img_name)\n",
    "        gaze_name = os.path.join(self.data_path+\"\\predict_heatmap\", self.data_frame['imgID'][idx])\n",
    "        gaze_img = Image.open(gaze_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            depth = transform2(depth)\n",
    "            gaze_img = transform2(gaze_img)\n",
    "        gaze = self.data_frame['gaze'][idx]   \n",
    "        gaze_idx = self.gaze_to_int[gaze]\n",
    "\n",
    "        # Convert gaze_idx to one-hot encoded vector\n",
    "        one_hot_gaze = torch.zeros(self.num_classes)\n",
    "        one_hot_gaze[gaze_idx] = 1\n",
    "        return image, depth,gaze_img, one_hot_gaze\n",
    "\n",
    "# Create datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming that your CSV file has two columns: 'imgID' for image file names and 'gaze' for gaze labels\n",
    "data_frame = pd.read_csv('C:\\Datasets\\labels_and_features_TRAIN.csv')\n",
    "\n",
    " \n",
    "\n",
    "train_dataset = GazeDataset(csv_file='C:\\Datasets\\labels_and_features_TRAIN.csv', data_path=data_path, transform=transform)\n",
    "valid_dataset = GazeDataset(csv_file='C:\\Datasets\\labels_and_features_VAL.csv', data_path=data_path, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=False, num_workers=0)\n",
    "\n",
    "# Load a pre-trained ResNet-18 model and modify the final layer\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "#model=model2\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00001)\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Define your desired number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(\"start epoch :\"+str(epoch))\n",
    "    # Add a progress bar for the training loop\n",
    "\n",
    "    for images,depth, gaze, gazes in tqdm(train_loader): # train_progress_bar:\n",
    "\n",
    "        inputs = torch.cat([gaze], dim=1)\n",
    "        # print(inputs.shape)\n",
    "        inputs, gazes = inputs.to(device), torch.argmax(gazes, dim=1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, gazes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Update the progress bar\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_epochs == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_dir, f\"epoch_{epoch+1}.ckpt\"))\n",
    "\n",
    "    # Validation loop\n",
    "    if (epoch + 1) % validate_epochs == 0:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Add a progress bar for the validation loop\n",
    "        valid_progress_bar = tqdm(valid_loader, desc='Validating', leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, depth, gaze, gazes in valid_progress_bar:\n",
    "                inputs = torch.cat([gaze ], dim=1)\n",
    "                inputs, gazes = inputs.to(device), torch.argmax(gazes, dim=1).to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += gazes.size(0)\n",
    "                correct += (predicted == gazes).sum().item()\n",
    "\n",
    "        print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121cfa9e-02e0-4dae-94fa-f80c324644be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of (720, 1280)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "filepath=r\"C:\\Datasets\\Engagement\\npy\\000000.npy\"\n",
    "data = np.load(filepath)\n",
    "print(f\"Size of {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d04ca71-1b6c-462c-aec8-05fe7ce7a4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [4, 4, 4, ..., 0, 0, 0],\n",
       "       [4, 4, 4, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9022f8af-970d-4bd6-b016-17914955e834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b37697-d030-44cd-adee-2c06d9880bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
