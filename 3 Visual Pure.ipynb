{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3198b8b4-0c68-4302-9dfc-ed1eca03af85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:35:52.442910500Z",
     "start_time": "2023-10-26T14:35:52.390910500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class GazeDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            data_path (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        # Create a dictionary mapping each unique gaze value to a unique integer\n",
    "        self.gaze_to_int = {gaze: idx for idx, gaze in enumerate(self.data_frame['gaze'].unique())}\n",
    "        self.num_classes = len(self.gaze_to_int)\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_path, self.data_frame['imgID'][idx])  # Assuming imgID is in the first column\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        gaze = self.data_frame['gaze'][idx]   # Assuming gaze is in the second column\n",
    "        gaze_idx = self.gaze_to_int[gaze]\n",
    "\n",
    "        # Convert gaze_idx to one-hot encoded vector\n",
    "        one_hot_gaze = torch.zeros(self.num_classes)\n",
    "        one_hot_gaze[gaze_idx] = 1\n",
    "        return image, one_hot_gaze\n",
    "\n",
    "# Create datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming that your CSV file has two columns: 'imgID' for image file names and 'gaze' for gaze labels\n",
    "data_frame = pd.read_csv('D:\\Datasets\\Talis_frames15_v2\\labels_and_features_TRAIN.csv')\n",
    "\n",
    "# Count the frequency of each class\n",
    "class_sample_counts = data_frame['gaze'].value_counts().sort_index().to_numpy()\n",
    "# Compute weights for each class\n",
    "weights = 1.0 / class_sample_counts\n",
    "# Create a weight for each sample in the dataset\n",
    "sample_weights = weights[data_frame['gaze'].replace({gaze: idx for idx, gaze in enumerate(data_frame['gaze'].unique())}).to_numpy()]\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = GazeDataset(csv_file='D:\\Datasets\\Talis_frames15_v2\\labels_and_features_TRAIN.csv', data_path=data_path, transform=transform)\n",
    "valid_dataset = GazeDataset(csv_file='D:\\Datasets\\Talis_frames15_v2\\labels_and_features_VAL.csv', data_path=data_path, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, sampler=sampler, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "31ceae42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:35:52.454910400Z",
     "start_time": "2023-10-26T14:35:52.439910200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0, 2: 1, 1: 2, 4: 3}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.gaze_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0af5aacc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:35:52.500910200Z",
     "start_time": "2023-10-26T14:35:52.455910300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec59b46b-ff1c-4783-b801-859ae37c27a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:35:52.504910200Z",
     "start_time": "2023-10-26T14:35:52.471910400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "class_counts = train_dataset.data_frame['gaze'].value_counts().to_dict()\n",
    "total_samples = sum(class_counts.values())\n",
    "class_weights = [total_samples/class_counts[class_] for class_ in sorted(train_dataset.gaze_to_int)]\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "criterion2 = nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a50a18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Train Depth+Img Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d71d6c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T15:12:16.129668200Z",
     "start_time": "2023-10-26T15:08:02.233606500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch :0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [01:23<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.256945919680905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   5%|▌         | 1/19 [00:01<00:18,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|█         | 2/19 [00:02<00:17,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 3/19 [00:03<00:17,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  21%|██        | 4/19 [00:04<00:16,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  26%|██▋       | 5/19 [00:05<00:15,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  32%|███▏      | 6/19 [00:06<00:13,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  37%|███▋      | 7/19 [00:07<00:12,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 8/19 [00:08<00:11,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 9/19 [00:10<00:12,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  53%|█████▎    | 10/19 [00:11<00:11,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  58%|█████▊    | 11/19 [00:12<00:09,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  63%|██████▎   | 12/19 [00:13<00:07,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  68%|██████▊   | 13/19 [00:14<00:06,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  74%|███████▎  | 14/19 [00:15<00:05,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  79%|███████▉  | 15/19 [00:16<00:04,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  84%|████████▍ | 16/19 [00:17<00:03,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  89%|████████▉ | 17/19 [00:18<00:02,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  95%|█████████▍| 18/19 [00:19<00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 6, 224, 224])\n",
      "Validation Accuracy: 26.94214876033058%\n",
      "start epoch :1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [01:20<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.1061678157224284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   5%|▌         | 1/19 [00:00<00:17,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  11%|█         | 2/19 [00:01<00:16,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  16%|█▌        | 3/19 [00:03<00:16,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  21%|██        | 4/19 [00:04<00:15,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  26%|██▋       | 5/19 [00:05<00:14,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  32%|███▏      | 6/19 [00:06<00:13,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  37%|███▋      | 7/19 [00:07<00:12,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  42%|████▏     | 8/19 [00:08<00:11,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  47%|████▋     | 9/19 [00:09<00:10,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  53%|█████▎    | 10/19 [00:10<00:09,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  58%|█████▊    | 11/19 [00:11<00:08,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  63%|██████▎   | 12/19 [00:12<00:07,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  68%|██████▊   | 13/19 [00:13<00:06,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  74%|███████▎  | 14/19 [00:14<00:05,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  79%|███████▉  | 15/19 [00:15<00:04,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  84%|████████▍ | 16/19 [00:16<00:03,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  89%|████████▉ | 17/19 [00:17<00:02,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  95%|█████████▍| 18/19 [00:18<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 6, 224, 224])\n",
      "Validation Accuracy: 25.950413223140497%\n",
      "start epoch :2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 47/77 [00:50<00:32,  1.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    125\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 127\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Update the progress bar\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch=32\n",
    "data_path = r\"D:\\Datasets\\Talis_frames15_v2\"  # Provide your data path here\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "validate_epochs = 1  # Example: validate every epoch\n",
    "save_epochs = 2     # Example: save the model every 2 epochs\n",
    "ckpt_dir = r\"D:\\Datasets\\Talis_frames15_v2\\checkpoints\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Randomly flip the image vertically\n",
    "    transforms.RandomRotation(degrees=15),  # Randomly rotate the image by up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly change the brightness, contrast, saturation, and hue\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.75, 1.33)),  # Randomly crop and resize the image\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "class GazeDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            data_path (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        # Create a dictionary mapping each unique gaze value to a unique integer\n",
    "        self.gaze_to_int = {gaze: idx for idx, gaze in enumerate(self.data_frame['gaze'].unique())}\n",
    "        self.num_classes = len(self.gaze_to_int)\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_path, self.data_frame['imgID'][idx])  # Assuming imgID is in the first column\n",
    "        image = Image.open(img_name)\n",
    "        dep_name = os.path.join(self.data_path+\"_depth\", self.data_frame['imgID'][idx])\n",
    "        depth = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            depth = transform2(depth)\n",
    "        gaze = self.data_frame['gaze'][idx]   # Assuming gaze is in the second column\n",
    "        gaze_idx = self.gaze_to_int[gaze]\n",
    "\n",
    "        # Convert gaze_idx to one-hot encoded vector\n",
    "        one_hot_gaze = torch.zeros(self.num_classes)\n",
    "        one_hot_gaze[gaze_idx] = 1\n",
    "        return image, depth, one_hot_gaze\n",
    "\n",
    "# Create datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming that your CSV file has two columns: 'imgID' for image file names and 'gaze' for gaze labels\n",
    "data_frame = pd.read_csv('D:\\Datasets\\Talis_frames15_v2\\labels_and_features_TRAIN.csv')\n",
    "\n",
    "# Count the frequency of each class\n",
    "class_sample_counts = data_frame['gaze'].value_counts().sort_index().to_numpy()\n",
    "# Compute weights for each class\n",
    "weights = 1.0 / class_sample_counts\n",
    "# Create a weight for each sample in the dataset\n",
    "sample_weights = weights[data_frame['gaze'].replace({gaze: idx for idx, gaze in enumerate(data_frame['gaze'].unique())}).to_numpy()]\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = GazeDataset(csv_file='D:\\Datasets\\Talis_frames15_v2\\labels_and_features_TRAIN.csv', data_path=data_path, transform=transform)\n",
    "valid_dataset = GazeDataset(csv_file='D:\\Datasets\\Talis_frames15_v2\\labels_and_features_VAL.csv', data_path=data_path, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, sampler=sampler, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=False, num_workers=0)\n",
    "\n",
    "# Load a pre-trained ResNet-18 model and modify the final layer\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "model.conv1 = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "#model=model2\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00001)\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Define your desired number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(\"start epoch :\"+str(epoch))\n",
    "    # Add a progress bar for the training loop\n",
    "\n",
    "    for images,depth, gazes in tqdm(train_loader): # train_progress_bar:\n",
    "        inputs = torch.cat([images, depth], dim=1)\n",
    "        # print(inputs.shape)\n",
    "        inputs, gazes = inputs.to(device), torch.argmax(gazes, dim=1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, gazes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Update the progress bar\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_epochs == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_dir, f\"epoch_{epoch+1}.ckpt\"))\n",
    "\n",
    "    # Validation loop\n",
    "    if (epoch + 1) % validate_epochs == 0:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Add a progress bar for the validation loop\n",
    "        valid_progress_bar = tqdm(valid_loader, desc='Validating', leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, depth, gazes in valid_progress_bar:\n",
    "                inputs = torch.cat([images, depth], dim=1)\n",
    "                inputs, gazes = inputs.to(device), torch.argmax(gazes, dim=1).to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += gazes.size(0)\n",
    "                correct += (predicted == gazes).sum().item()\n",
    "\n",
    "        print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6314c9-40ba-4bc3-875f-e580aa89f1c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:53:57.296589500Z",
     "start_time": "2023-10-26T14:53:57.265589600Z"
    }
   },
   "source": [
    "# Train Depth+Img+Head+Eye Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad723b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch=32\n",
    "data_path = r\"D:\\Datasets\\Talis_frames15_v2\"  # Provide your data path here\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "validate_epochs = 1  # Example: validate every epoch\n",
    "save_epochs = 2     # Example: save the model every 2 epochs\n",
    "ckpt_dir = r\"D:\\Datasets\\Talis_frames15_v2\\checkpoints\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip the image horizontally\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Randomly flip the image vertically\n",
    "    transforms.RandomRotation(degrees=15),  # Randomly rotate the image by up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly change the brightness, contrast, saturation, and hue\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.75, 1.33)),  # Randomly crop and resize the image\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "class GazeDataset(Dataset):\n",
    "    def __init__(self, csv_file, data_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            data_path (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        # Create a dictionary mapping each unique gaze value to a unique integer\n",
    "        self.gaze_to_int = {gaze: idx for idx, gaze in enumerate(self.data_frame['gaze'].unique())}\n",
    "        self.num_classes = len(self.gaze_to_int)\n",
    "        self.head_\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_path, self.data_frame['imgID'][idx])  # Assuming imgID is in the first column\n",
    "        image = Image.open(img_name)\n",
    "        head=\n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            depth = transform2(depth)\n",
    "        gaze = self.data_frame['gaze'][idx]   # Assuming gaze is in the second column\n",
    "        gaze_idx = self.gaze_to_int[gaze]\n",
    "\n",
    "        # Convert gaze_idx to one-hot encoded vector\n",
    "        one_hot_gaze = torch.zeros(self.num_classes)\n",
    "        one_hot_gaze[gaze_idx] = 1\n",
    "        return image, depth, one_hot_gaze\n",
    "\n",
    "# Create datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming that your CSV file has two columns: 'imgID' for image file names and 'gaze' for gaze labels\n",
    "data_frame = pd.read_csv('D:\\Datasets\\Talis_frames15_v2\\labels_and_features_TRAIN.csv')\n",
    "\n",
    "# Count the frequency of each class\n",
    "class_sample_counts = data_frame['gaze'].value_counts().sort_index().to_numpy()\n",
    "# Compute weights for each class\n",
    "weights = 1.0 / class_sample_counts\n",
    "# Create a weight for each sample in the dataset\n",
    "sample_weights = weights[data_frame['gaze'].replace({gaze: idx for idx, gaze in enumerate(data_frame['gaze'].unique())}).to_numpy()]\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = GazeDataset(csv_file='D:\\Datasets\\Talis_frames15_v2\\labels_and_features_TRAIN.csv', data_path=data_path, transform=transform)\n",
    "valid_dataset = GazeDataset(csv_file='D:\\Datasets\\Talis_frames15_v2\\labels_and_features_VAL.csv', data_path=data_path, transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch, sampler=sampler, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch, shuffle=False, num_workers=0)\n",
    "\n",
    "# Load a pre-trained ResNet-18 model and modify the final layer\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "model.conv1 = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "#model=model2\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.00001)\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Define your desired number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(\"start epoch :\"+str(epoch))\n",
    "    # Add a progress bar for the training loop\n",
    "\n",
    "    for images,depth, gazes in tqdm(train_loader): # train_progress_bar:\n",
    "        inputs = torch.cat([images, depth], dim=1)\n",
    "        # print(inputs.shape)\n",
    "        inputs, gazes = inputs.to(device), torch.argmax(gazes, dim=1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, gazes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Update the progress bar\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % save_epochs == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_dir, f\"epoch_{epoch+1}.ckpt\"))\n",
    "\n",
    "    # Validation loop\n",
    "    if (epoch + 1) % validate_epochs == 0:\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # Add a progress bar for the validation loop\n",
    "        valid_progress_bar = tqdm(valid_loader, desc='Validating', leave=False)\n",
    "        with torch.no_grad():\n",
    "            for images, depth, gazes in valid_progress_bar:\n",
    "                inputs = torch.cat([images, depth], dim=1)\n",
    "                inputs, gazes = inputs.to(device), torch.argmax(gazes, dim=1).to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += gazes.size(0)\n",
    "                correct += (predicted == gazes).sum().item()\n",
    "\n",
    "        print(f\"Validation Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d8f86-72e9-4892-9a3a-ef3b5c545def",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Tabular Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ed90d-7ab8-4445-bbc8-b4260d63c4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
